{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xT7MKZuMRaCg"
   },
   "source": [
    "# Sentiment Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wq4RCyyPSYRp"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NGCtiXUhSWss"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "\n",
    "np_load_old = np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "vocab_size = 10000 #vocab size\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size) # vocab_size is no.of words to consider from the dataset, ordering based on frequency.\n",
    "np.load = np_load_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2ZAS6f4hCHOZ",
    "outputId": "3d328623-298d-4672-ed7b-6d33e2f5a554"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uY1BMefoCJ3d",
    "outputId": "b18a0b9f-8e60-49e3-9224-e32a98aee140"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "aZO-3S69TVpA",
    "outputId": "9e317066-807c-42db-f124-f02b7c32fe88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Words---\n",
      "? french horror cinema has seen something of a revival over the last couple of years with great films such as inside and ? romance ? on to the scene ? ? the revival just slightly but stands head and shoulders over most modern horror titles and is surely one of the best french horror films ever made ? was obviously shot on a low budget but this is made up for in far more ways than one by the originality of the film and this in turn is ? by the excellent writing and acting that ensure the film is a winner the plot focuses on two main ideas prison and black magic the central character is a man named ? sent to prison for fraud he is put in a cell with three others the quietly insane ? body building ? marcus and his retarded boyfriend daisy after a short while in the cell together they stumble upon a hiding place in the wall that contains an old ? after ? part of it they soon realise its magical powers and realise they may be able to use it to break through the prison walls br br black magic is a very interesting topic and i'm actually quite surprised that there aren't more films based on it as there's so much scope for things to do with it it's fair to say that ? makes the best of it's ? as despite it's ? the film never actually feels restrained and manages to flow well throughout director eric ? provides a great atmosphere for the film the fact that most of it takes place inside the central prison cell ? that the film feels very claustrophobic and this immensely benefits the central idea of the prisoners wanting to use magic to break out of the cell it's very easy to get behind them it's often said that the unknown is the thing that really ? people and this film proves that as the director ? that we can never really be sure of exactly what is round the corner and this helps to ensure that ? actually does manage to be quite frightening the film is memorable for a lot of reasons outside the central plot the characters are all very interesting in their own way and the fact that the book itself almost takes on its own character is very well done anyone worried that the film won't deliver by the end won't be disappointed either as the ending both makes sense and manages to be quite horrifying overall ? is a truly great horror film and one of the best of the decade highly recommended viewing\n",
      "---Label---\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Creating word index:\n",
    "word_index = imdb.get_word_index()\n",
    "word_and_word_index = dict(\n",
    "[(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "decoded_review = ' '.join(\n",
    "[word_and_word_index.get(i - 3, '?') for i in x_train[10]])\n",
    "print('---Words---')\n",
    "print(decoded_review)\n",
    "print('---Label---')\n",
    "print(y_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GhX4RL8iTYey",
    "outputId": "d00335a5-ed51-48be-f707-c6ccc7f4fec8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88584"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding the lenght of the word index\n",
    "len(word_and_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fCPC_WN-eCyw"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "vocab_size = 10000 #vocab size\n",
    "maxlen = 300  #number of word used from each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NNbxvy7bCTIW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qMEsHYrWxdtk"
   },
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h0g381XzeCyz"
   },
   "outputs": [],
   "source": [
    "#load dataset as a list of ints\n",
    "# (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "#make all sequences of the same length\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test =  pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "UcsfvEFfCjdo",
    "outputId": "564e3224-16c4-4c27-8270-899c5e065173"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...   19  178   32]\n",
      " [   0    0    0 ...   16  145   95]\n",
      " [   0    0    0 ...    7  129  113]\n",
      " ...\n",
      " [   0    0    0 ...    4 3586    2]\n",
      " [   0    0    0 ...   12    9   23]\n",
      " [   0    0    0 ...  204  131    9]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "4RGIAzfSCm03",
    "outputId": "eb80cc3a-f0ac-4779-abd0-4c65c11d477b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...   14    6  717]\n",
      " [   0    0    0 ...  125    4 3077]\n",
      " [1239 5189  137 ...    9   57  975]\n",
      " ...\n",
      " [   0    0    0 ...   21  846 5518]\n",
      " [   0    0    0 ... 2302    7  470]\n",
      " [   0    0    0 ...   34 2005 2643]]\n"
     ]
    }
   ],
   "source": [
    "print(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dybtUgUReCy8"
   },
   "source": [
    "## Build Keras Embedding Layer Model\n",
    "We can think of the Embedding layer as a dicionary that maps a index assigned to a word to a word vector. This layer is very flexible and can be used in a few ways:\n",
    "\n",
    "* The embedding layer can be used at the start of a larger deep learning model. \n",
    "* Also we could load pre-train word embeddings into the embedding layer when we create our model.\n",
    "* Use the embedding layer to train our own word2vec models.\n",
    "\n",
    "The keras embedding layer doesn't require us to onehot encode our words, instead we have to give each word a unqiue intger number as an id. For the imdb dataset we've loaded this has already been done, but if this wasn't the case we could use sklearn [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A5OLM4eBeCy9"
   },
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout,GRU\n",
    "embedding_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TxNDNhrseCzA"
   },
   "outputs": [],
   "source": [
    "#Adding the LSTM details:\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_size, input_length=maxlen))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "L3CSVVPPeCzD",
    "outputId": "49f8e716-d298-4040-b3fd-2fd6f9f74505"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 300, 32)           320000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 373,301\n",
      "Trainable params: 373,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Compiling the model:\n",
    "model.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "yE8ffTF8DgBL",
    "outputId": "8051ccd8-836a-460e-de44-4a80716e1888"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24936 samples, validate on 64 samples\n",
      "Epoch 1/10\n",
      "24936/24936 [==============================] - 263s 11ms/step - loss: 0.5096 - acc: 0.7438 - val_loss: 0.2695 - val_acc: 0.9062\n",
      "Epoch 2/10\n",
      "24936/24936 [==============================] - 267s 11ms/step - loss: 0.2752 - acc: 0.8902 - val_loss: 0.2244 - val_acc: 0.9219\n",
      "Epoch 3/10\n",
      "24936/24936 [==============================] - 266s 11ms/step - loss: 0.2378 - acc: 0.9040 - val_loss: 0.2277 - val_acc: 0.9219\n",
      "Epoch 4/10\n",
      "24936/24936 [==============================] - 256s 10ms/step - loss: 0.1663 - acc: 0.9404 - val_loss: 0.2496 - val_acc: 0.8906\n",
      "Epoch 5/10\n",
      "24936/24936 [==============================] - 261s 10ms/step - loss: 0.1316 - acc: 0.9562 - val_loss: 0.3382 - val_acc: 0.8906\n",
      "Epoch 6/10\n",
      "24936/24936 [==============================] - 264s 11ms/step - loss: 0.1086 - acc: 0.9627 - val_loss: 0.2967 - val_acc: 0.8906\n",
      "Epoch 7/10\n",
      "24936/24936 [==============================] - 264s 11ms/step - loss: 0.0788 - acc: 0.9737 - val_loss: 0.3108 - val_acc: 0.9219\n",
      "Epoch 8/10\n",
      "24936/24936 [==============================] - 265s 11ms/step - loss: 0.0661 - acc: 0.9781 - val_loss: 0.3364 - val_acc: 0.9219\n",
      "Epoch 9/10\n",
      "24936/24936 [==============================] - 264s 11ms/step - loss: 0.0552 - acc: 0.9824 - val_loss: 0.2616 - val_acc: 0.9375\n",
      "Epoch 10/10\n",
      "24936/24936 [==============================] - 252s 10ms/step - loss: 0.0505 - acc: 0.9842 - val_loss: 0.3329 - val_acc: 0.9375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdbc330c650>"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Running 10 epochs by keeping batch size = 64:\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "x_test, y_test = x_train[:batch_size], y_train[:batch_size]\n",
    "x_train2, y_train2 = x_train[batch_size:], y_train[batch_size:]\n",
    "model.fit(x_train2, y_train2, validation_data=(x_test, y_test), batch_size=batch_size, epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "5o6zU2YTQIkL",
    "outputId": "3eea8681-8872-46b5-bfaa-b8a5c1a9b38e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "64/64 [==============================] - 0s 4ms/step\n",
      "('Test Score:', 0.3328913748264313)\n",
      "('Test Accuracy:', 0.9375)\n"
     ]
    }
   ],
   "source": [
    "#Calculating the score and accuracy:\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test Score:', score)\n",
    "print('Test Accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UG8uPJABet4E"
   },
   "outputs": [],
   "source": [
    "#Importing word tokenizer:\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Yk9HJcK5ewJD",
    "outputId": "0fff495c-9e9a-4043-8011-f6c352b0cf21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "_QANbGbXeyfZ",
    "outputId": "f94d743f-791e-4382-bcb5-64f39ec7289b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qIVmCvnke0xO"
   },
   "outputs": [],
   "source": [
    "review_lines = list()\n",
    "lines = word_and_word_index.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RLCBelque3bL"
   },
   "outputs": [],
   "source": [
    "#Removing special characters:\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Tkg4N72e6eQ"
   },
   "outputs": [],
   "source": [
    "for line in lines:\n",
    "  tokens = word_tokenize(line)\n",
    "  #convert to lowercase\n",
    "  tokens = [w.lower() for w in tokens]\n",
    "  #remove puchation and numbers from each word\n",
    "  words = [remove_special_characters(w,remove_digits=True) for w in tokens]\n",
    "  #filter out stopwords\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  words = [w for w in words if not w in stop_words]\n",
    "  review_lines.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EMYIvP5_fEHb",
    "outputId": "d448eac6-9569-46b2-ccab-83a975df8b88"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88584"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating length of review lines:\n",
    "len(review_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aH2zBoORfHTI"
   },
   "outputs": [],
   "source": [
    "#creating and training word2vec model:\n",
    "model = gensim.models.Word2Vec(sentences = review_lines, size=embedding_size, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LZiq703efJ-k",
    "outputId": "6ebbb50e-1265-4ea2-e2ae-cf0421157c08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabilary size : 73723\n"
     ]
    }
   ],
   "source": [
    "words = list(model.wv.vocab)\n",
    "print(\"vocabilary size : %d\"%len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "qGC5KFOTfMY3",
    "outputId": "6aa5a778-499b-40df-b83d-e24ee4ff55e9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'convents', 0.6671054363250732),\n",
       " (u'shriekfest', 0.6665854454040527),\n",
       " (u'quarrels', 0.6511059999465942),\n",
       " (u'titus', 0.629612147808075),\n",
       " (u'rouncewell', 0.6137033700942993),\n",
       " (u'fool', 0.6046589612960815),\n",
       " (u'racy', 0.6045066118240356),\n",
       " (u'dandies', 0.6043086647987366),\n",
       " (u'wowser', 0.5990558862686157),\n",
       " (u'normalos', 0.5986173152923584)]"
      ]
     },
     "execution_count": 63,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive = \"terrible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "YGL7zT6YfQyo",
    "outputId": "30aaaa90-4702-4ee3-b920-1286a423c0b8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "#Saving the model:\n",
    "filename = \"imdb_embedding_word2vec.txt\"\n",
    "model.wv.save_word2vec_format(filename,binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_iXLV6oufTe8"
   },
   "outputs": [],
   "source": [
    "#creating embedding matrix:\n",
    "import os\n",
    "embedding_index = {}\n",
    "f = open(os.path.join('','imdb_embedding_word2vec.txt'))\n",
    "for line in f:\n",
    "  values = line.split()\n",
    "  word = values[0]\n",
    "  coefs = np.asarray(values[1:])\n",
    "  embedding_index[word]=coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hanblEMMfWSj"
   },
   "outputs": [],
   "source": [
    "num_words = len(word_and_word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, embedding_size))\n",
    "for word, i in word_and_word_index.items():\n",
    "  if i > num_words:\n",
    "    continue\n",
    "  embedding_vector = embedding_index.get(word)\n",
    "  if embedding_vector is not None:\n",
    "    embedding_matix[i]=embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "XOFJzuTUfY7o",
    "outputId": "8c637557-35fd-4ce5-ae63-6aea051de0d3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0112 16:02:01.241247 140585223972736 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 300, 32)           2834720   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 32)                6240      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,840,993\n",
      "Trainable params: 6,273\n",
      "Non-trainable params: 2,834,720\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Bulding model using Pre-trained Embedding\n",
    "model=Sequential()\n",
    "embedding_layer = Embedding(num_words, embedding_size, weights=[embedding_matrix], input_length=maxlen, trainable = False)\n",
    "model.add(embedding_layer)\n",
    "model.add(GRU(units=32,dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "u_AjN5zmfnuv",
    "outputId": "ca09360d-5cdf-474b-a793-fc1cbbf3d7e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24936 samples, validate on 64 samples\n",
      "Epoch 1/10\n",
      "24936/24936 [==============================] - 226s 9ms/step - loss: 0.6932 - acc: 0.5026 - val_loss: 0.7004 - val_acc: 0.3906\n",
      "Epoch 2/10\n",
      "24936/24936 [==============================] - 224s 9ms/step - loss: 0.6933 - acc: 0.4986 - val_loss: 0.6948 - val_acc: 0.3906\n",
      "Epoch 3/10\n",
      "24936/24936 [==============================] - 237s 9ms/step - loss: 0.6933 - acc: 0.4968 - val_loss: 0.6918 - val_acc: 0.6094\n",
      "Epoch 4/10\n",
      "24936/24936 [==============================] - 241s 10ms/step - loss: 0.6932 - acc: 0.4980 - val_loss: 0.6932 - val_acc: 0.3906\n",
      "Epoch 5/10\n",
      "24936/24936 [==============================] - 242s 10ms/step - loss: 0.6932 - acc: 0.5021 - val_loss: 0.6964 - val_acc: 0.3906\n",
      "Epoch 6/10\n",
      "24936/24936 [==============================] - 234s 9ms/step - loss: 0.6932 - acc: 0.4973 - val_loss: 0.6921 - val_acc: 0.6094\n",
      "Epoch 7/10\n",
      "24936/24936 [==============================] - 241s 10ms/step - loss: 0.6932 - acc: 0.4991 - val_loss: 0.6933 - val_acc: 0.3906\n",
      "Epoch 8/10\n",
      "24936/24936 [==============================] - 236s 9ms/step - loss: 0.6932 - acc: 0.5003 - val_loss: 0.6940 - val_acc: 0.3906\n",
      "Epoch 9/10\n",
      "24936/24936 [==============================] - 232s 9ms/step - loss: 0.6932 - acc: 0.4931 - val_loss: 0.6931 - val_acc: 0.6094\n",
      "Epoch 10/10\n",
      "24936/24936 [==============================] - 232s 9ms/step - loss: 0.6932 - acc: 0.4964 - val_loss: 0.6925 - val_acc: 0.6094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdbbedc4fd0>"
      ]
     },
     "execution_count": 70,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compiling the model:\n",
    "model.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])\n",
    "\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "x_test, y_test = x_train[:batch_size], y_train[:batch_size]\n",
    "x_train2, y_train2 = x_train[batch_size:], y_train[batch_size:]\n",
    "model.fit(x_train2, y_train2, validation_data=(x_test, y_test), batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Igq8Qm8GeCzG"
   },
   "source": [
    "## Retrive the output of each layer in keras for a given single test sample from the trained model you built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "Ymc3Dm-Bfb2x",
    "outputId": "bf7cfd41-5d36-4057-838e-bb347c29dfd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[array([[[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)], [array([[ 0.00588785, -0.00646025, -0.00996825, -0.00576604, -0.00424212,\n",
      "         0.00093046, -0.00146908,  0.00016623,  0.00988423, -0.01017534,\n",
      "        -0.00857754,  0.00288379, -0.00546875,  0.00428849, -0.00956388,\n",
      "        -0.0072801 , -0.01553406,  0.02446816,  0.00184386,  0.01164285,\n",
      "         0.00395406, -0.00380243, -0.00530218,  0.00203009,  0.00273696,\n",
      "         0.00494729, -0.00436864,  0.00285287,  0.01668944,  0.00939758,\n",
      "        -0.00077096,  0.01227883]], dtype=float32)], [array([[0.4983442]], dtype=float32)]]\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "inp = model.input                                           # input placeholder\n",
    "outputs = [layer.output for layer in model.layers]          # all layer outputs\n",
    "functors = [K.function([inp, K.learning_phase()], [out]) for out in outputs]    # evaluation functions\n",
    "\n",
    "#Testing the num words:\n",
    "test = np.random.random(num_words)[np.newaxis,...]\n",
    "layer_outs = [func([test, 1.]) for func in functors]\n",
    "print layer_outs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SeqNLP_Project1_Questions_Assignment_Naveen.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
